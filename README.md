# NGS Neturalization Assay Pipeline

![License](https://img.shields.io/github/license/matsengrp/multidms)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Snakemake](https://img.shields.io/badge/snakemake-â‰¥7.30-brightgreen.svg?style=flat)](https://snakemake.readthedocs.io)

---

This is the analysis pipeline for analyzing barcode counts for the high-throughput neutralization assay developed by the [Bloom Lab](https://research.fredhutch.org/bloom/en.html?gad_source=1&gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoWkKHOThcnTs5JsV0pxNgtbnBOXKsdcf_JQ2b7Ja7t_D0zQRzZTLoaAothEALw_wcB).

## Running the Analysis

First create the **main** `conda` environment: 

```
conda env create -f environment.yml
```

Then, once the environment has been created, activate it: 

```
conda activate ngs-neuts
```

*\*Note, if you've already created the environment, all you need to do is activate it using the command above.*

To run the `snakemake` pipeline locally, simply run the following command from the top of the repo:

```
snakemake -j 4 --use-conda --conda-prefix env
```

To run the `snakemake` pipeline using `slurm`, use the bash script ([`run_analysis.bash`](/run_analysis.bash))

```
sbatch run_analysis.bash
```

The output of the pipeline will be generated in the [`results`](/results/) directory. The output of `slurm` is located in a `tmp` directory that's overwritten for each run of the pipeline.


## Input Data Requirements 

There are **three** mandatory input files that are needed to run this pipeline. All of these files should be located in the [data directory](/data/) and the names of each should be specified in the [`config.yml`](/config.yml) file. I'll describe each of these files in detail below: 

1. [`barcode_runs.csv`](/data/barcode_runs.csv): This table contains information for each individually sequencened well. You can specify the name of this file in the `baracode_runs` field of the [`config.yml`](/config.yml) file. The required columns (these must be the names) are: 
    - `library`: The name of the barcode library 
    - `standard_set`: The name of the neutraliztaion standard set
    - `date`: The date of the experiment, must be in the format YYMMDD (i.e. 231101)
    - `plate`: The name of the plate, it must be unique
    - `fastq`: The absolute path to the sequencing runs
    - `serum`: A unique identifier for each serum sample run
    - `individual`: A unique identifier for each individual from which serum was collected

    Other than these required columns, you must specify a list of column names that can be combined to create a **unique** sample name for each column. You can specify these columns and their order in the `id_columns` field of the [`config.yml`](/config.yml) file.

2. [`neutralization_standards.csv`](/data/neutralization_standards.csv): This table contains the barcodes for the neutralization standards. You can specify this file in the `neut_standards` field of the [`config.yml`](/config.yml) file. There are only two required columns: 
    - `barcodes`: The barcodes of the neutralization standard set
    - `standard_set`: The name of the neutralization standard set that correspond to the `standard_set` for the sequenced well

3. [`strain_to_barcode.csv`](/data/strain_to_barcode.csv): This table contains the barcodes for each library. You can specify this file in the `strain_to_barcode` field of the [`config.yml`](/config.yml) file. The required columns are: 
    - `barcodes`: The barcodes in the library
    - `library`: The name of the library; it must correspond to the name in the corresponding `barcode_runs.csv` file
    - `strain`: The name of the strain that a barcode corresponds to


## Organization of the Repo

*This repository is organized as follows:*

- [data](/data/) This directory contains the input data for the analysis:
    - [`barcode_runs.csv`](/data/barcode_runs.csv) Contains information for each antibody and serum run and corresponding `fastq` file
    - [`neutralization_standards.csv`](/data/neutralization_standards.csv) Contains the neutralization standard barcodes for the corresponding `standard_set` name
    - [`strain_to_barcode.csv`](/data/strain_to_barcode.csv) Contains the barcodes for a corresponding library and the variant/strain the a barcode corresponds to
- [workflow](/workflow/) This directory contains the code to run the analysis: 
    - [`Snakefile`](/workflow/Snakefile) The `snakemake` rules that run the analysis
    - [scripts](/workflow/scripts) All `python` scripts used by the analysis
    - [notebooks](/workflow/notebooks/) All `jupyter` notebooks used by the analysis
    - [envs](/workflow/envs/) `conda` environment files for specific rules 
- [results](/results/) This directory contains the results files generated by the analysis
    - [barcode_counts]() The barcode counts for each sample, and summary files for all samples and per plate
    - [fraction_infectivity] The fraction infectivity files calculated by plate
    - [selections]() The plots showing the curves used to calculate NT50s for each serum sample and the file containing the NT50s per strain for each sample
- [`environment.yml`](/environment.yml) Builds the main `conda` environment
- [`config.yml`](/config.yml) Configures the `snakemake` pipeline
- [`cluster.yml`](/cluster.yml) Specifies the properties of jobs submitted to the cluster via `slurm`
- [`run_analysis.bash`](/run_analysis.bash) A shell script to run the analysis with `slurm`



