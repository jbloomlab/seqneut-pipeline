# NGS Neturalization Assay Pipeline

![License](https://img.shields.io/github/license/matsengrp/multidms)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

---

This is the analysis pipeline for analyzing barcode counts for the high-throughput neutralization assay developed by the [Bloom Lab](https://research.fredhutch.org/bloom/en.html?gad_source=1&gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoWkKHOThcnTs5JsV0pxNgtbnBOXKsdcf_JQ2b7Ja7t_D0zQRzZTLoaAothEALw_wcB).

## Running the Analysis

First create the **main** `conda` environment: 

```
conda env create -f environment.yml
```

Then, once the environment has been created, activate it: 

```
conda activate ngs-neuts
```

*\*Note, if you've already created the environment, all you need to do is activate it using the command above.*

To run the `snakemake` pipeline locally, simply run the following command from the top of the repo:

```
snakemake -j 4 --use-conda --conda-prefix env
```

To run the `snakemake` pipeline using `slurm`, use the bash script ([`run_analysis.bash`](/run_analysis.bash))

```
sbatch run_analysis.bash
```

The output of the pipeline will be generated in the [`results`](/results/) directory. The output of `slurm` is located in a `tmp` directory that's overwritten for each run of the pipeline.

## Organization of the Repo

*This repository is organized as follows:*

- [data](/data/) This directory contains the input data for the analysis:
    - [`barcode_runs.csv`](/data/barcode_runs.csv) Contains information for each antibody and serum run and corresponding `fastq` file
    - [`neutralization_standards.csv`](/data/neutralization_standards.csv) Contains the neutralization standard barcodes for the corresponding `standard_set` name
    - [`strain_to_barcode.csv`](/data/strain_to_barcode.csv) Contains the barcodes for a corresponding library and the variant/strain the a barcode corresponds to
- [workflow](/workflow/) This directory contains the code to run the analysis: 
    - [`Snakefile`](/workflow/Snakefile) The `snakemake` rules that run the analysis
    - [scripts](/workflow/scripts) All `python` scripts used by the analysis
    - [notebooks](/workflow/notebooks/) All `jupyter` notebooks used by the analysis
    - [envs](/workflow/envs/) `conda` environment files for specific rules 
- [results](/results/) This directory contains the results files generated by the analysis
    - [barcode_counts]() The barcode counts for each sample
- [`environment.yml`](/environment.yml) Builds the main `conda` environment
- [`config.yml`](/config.yml) Configures the `snakemake` pipeline
- [`cluster.yml`](/cluster.yml) Specifies the properties of jobs submitted to the cluster via `slurm`
- [`run_analysis.bash`](/run_analysis.bash) A shell script to run the analysis with `slurm`



