# NGS Neturalization Assay Pipeline

This is the analysis pipeline for counting barcodes for the NGS neutralization assay developed by the [Bloom Lab](https://research.fredhutch.org/bloom/en.html?gad_source=1&gclid=Cj0KCQjwqP2pBhDMARIsAJQ0CzoWkKHOThcnTs5JsV0pxNgtbnBOXKsdcf_JQ2b7Ja7t_D0zQRzZTLoaAothEALw_wcB).

## Running the Analysis

First create the **main** `conda` environment: 

```
conda env create -f environment.yml
```

Then, once the environment has been created, activate it: 

```
conda activate ngs-neuts
```

*\*Note, if you've already created the environment, all you need to do is activate it using the command above.*

To run the `snakemake` pipeline locally, simply run the following command from the top of the repo:

```
snakemake -j 4 --use-conda --conda-prefix env
```

To run the `snakemake` pipeline using `slurm`, use the bash script ([`run_analysis.bash`](/run_analysis.bash))"

```
sbatch run_analysis.bash
```

The output of the pipeline will be generated in the [`results`](/results/) directory. The output of `slurm` is located in a `tmp` directory that's overwritten for each run of the pipeline.

## Organization of the Repo

*This repository is organized as follows:*

- [data](/data/) This directory contains the input data for the analysis
- [workflow](/workflow/) This directory contains the code to run the analysis: 
    - [Snakefile](/workflow/Snakefile) The `snakemake` rules that run the analysis
    - [scripts](/workflow/scripts) All `python` scripts used by the analysis
    - [notebooks](/workflow/notebooks/) All `jupyter` notebooks used by the analysis
    - [envs](/workflow/envs/) `conda` environment files for specific rules 
- [results](/results/) This directory contains the results files generated by the analysis
- [environment.yml](/environment.yml) Builds the main `conda` environment
- [config.yml](/config.yml) Configures the `snakemake` pipeline
- [cluster.yml](/cluster.yml) Specifies the properties of jobs submitted to the cluster via `slurm`
- [run_analysis.bash](/run_analysis.bash) A shell script to run the analysis with `slurm`


